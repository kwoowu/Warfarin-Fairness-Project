# Warfarin-Fairness-Project

Machine learning can be used in the healthcare field to analyze large sets of data in order to support decision-making and predict risk. However, in certain cases, the algorithm may perpetuate biases against various subpopulations because of biases in past decisions or lack of data diversity. I used publicly-sourced data on warfarin dosage and the OLS method of linear regression to train a model that can predict dosage and then test the model's accuracy. I then analyzed the model from the perspective of two different definitions of fairness: given a sensitive attribute that defines a protected group, (1) treatment fairness is measured by how similarly the decision algorithm treats subjects who are and aren't in the protected group and (2) outcome fairness measures how similar the outcomes are for subjects who are and aren't in the protected group, given that they received the same treatment. Finally, I adjusted my model in order to maximize fairness while maintaining a high level of accuracy through using testing other model-training methods and adjusting whether or not sensitive attributes (i.e. gender, race, ethnicity) are revealed during the training process.
